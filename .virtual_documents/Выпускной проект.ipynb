





import pandas as pd
from ydata_profiling import ProfileReport
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sqlalchemy import create_engine, text
import os

from sklearn.model_selection import (
    train_test_split, GridSearchCV, RandomizedSearchCV, 
    cross_val_score, cross_validate
)
from sklearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import (
    StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder
)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from catboost import CatBoostClassifier

from sklearn.metrics import (
    roc_auc_score, f1_score, accuracy_score, precision_score, recall_score,
    confusion_matrix, ConfusionMatrixDisplay, classification_report,
    roc_curve, precision_recall_curve, RocCurveDisplay, PrecisionRecallDisplay
)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

import phik
from phik import phik_matrix
from phik.report import plot_correlation_matrix

from scipy.stats import chi2_contingency
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

print("âœ… Ğ’ÑĞµ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñ‹ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹")


path_to_db = 'ds-plus-final.db'
engine = create_engine(f'sqlite:///{path_to_db}', echo=False)

RANDOM_STATE = 20226 








# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ² Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
def check_tables():
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ² Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
    try:
        # Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¸ÑĞºĞ° Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†
        query = """
        SELECT name, type 
        FROM sqlite_master 
        WHERE type IN ('table', 'view')
        ORDER BY type, name;
        """
        
        with engine.connect() as conn:
            result = conn.execute(text(query))
            tables = [row[0] for row in result]
            
        print("Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ² Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…:")
        for table in tables:
            print(f"- {table}")
            
        return tables
        
    except Exception as e:
        print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†: {e}")
        return []

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ¿ĞµÑ€ĞµĞ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾Ğ¹ Ñ Ğ½Ğ¸Ğ¼Ğ¸
available_tables = check_tables()

print(f"\nĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(available_tables)} Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†(Ñ‹)")





tables = ['contract', 'personal', 'internet', 'phone']
dataframes = {}

for table in tables:
    query = f'''
    SELECT *
    FROM {table}
    '''
    df = pd.read_sql_query(query, con=engine) # , index_col='customerID'
    dataframes[f'df_{table}'] = df  # ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ





df_contract = dataframes['df_contract']
df_personal = dataframes['df_personal']
df_internet = dataframes['df_internet']
df_phone = dataframes['df_phone']








# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
def preprocess(df):
    print('ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ²:')
    print(df.isna().sum())
    print('\nĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ²Ğ½Ñ‹Ñ… Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ²: ', df.duplicated().sum())
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¿Ğ¸Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 
    df_copy = df.copy()

    # ĞŸÑ€Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğº Ğ½Ğ¸Ğ¶Ğ½ĞµĞ¼Ñƒ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ñƒ Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ²
    for col in df_copy.select_dtypes(include=['object', 'category']).columns:
        df_copy[col] = df_copy[col].str.lower().str.replace(' ', '', regex=False)

    print(f'ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ¾Ğ² Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ²: {df_copy.duplicated().sum()}\n')

    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ ĞºĞ¾Ğ¿Ğ¸Ñ Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
    del df_copy

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ¾Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹
    for col in df.select_dtypes(include=['object', 'category']).columns:
        print(f'Ğ¡Ñ‚Ğ¾Ğ»Ğ±ĞµÑ†: {col} \nĞ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ°: {df[col].unique()}\n')
    

def plot_histogram_with_boxplot(df, col, target=None):
    """
    Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ³Ğ¸ÑÑ‚Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ Ğ¸ boxplot Ğ´Ğ»Ñ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°.
    
    Parameters:
    df (pandas.DataFrame): Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚.
    col (str): ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ°.
    target (str, optional): ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ° Ğ´Ğ»Ñ Ñ€Ğ°ÑĞºÑ€Ğ°ÑĞºĞ¸.
    """
    sns.set()
    fig, axes = plt.subplots(1, 2, figsize=(16, 4))
    
    # Ğ“Ğ¸ÑÑ‚Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°
    axes[0].set_title(f'Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ° {col}', fontsize=16)
    axes[0].set_ylabel('ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾', fontsize=14)
    sns.histplot(df, bins=20, kde=True, ax=axes[0], hue=target, x=col)
    
    # Boxplot
    axes[1].set_title(f'Ğ¯Ñ‰Ğ¸Ğº Ñ ÑƒÑĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ {col}', fontsize=16)
    sns.boxplot(data=df, ax=axes[1], y=col)
    axes[1].set_ylabel(col, fontsize=14)
    
    plt.show()


def plot_pivot_pie_chart(df, col):
    """
    Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ ĞºÑ€ÑƒĞ³Ğ¾Ğ²ÑƒÑ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°.
    
    Parameters:
    df (pandas.DataFrame): Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚.
    col (str): ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ°.
    """
    value_counts = df[col].value_counts()
    plt.figure(figsize=(8, 8))
    plt.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%', startangle=140)
    plt.title(f'Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ {col}', fontsize=16)
    plt.show()








df_contract.head()


df_contract.info() 



preprocess(df_contract)


# ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ğ¸Ğ½Ğ´ĞµĞºÑÑ‹ ÑÑ‚Ñ€Ğ¾Ğº Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² TotalCharges
problematic_mask = ~pd.to_numeric(df_contract['TotalCharges'], errors='coerce').notna()
problematic_indices = df_contract[problematic_mask].index

print(f"\nĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(problematic_indices)} ÑÑ‚Ñ€Ğ¾Ğº Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸:")
print(f"Ğ˜Ğ½Ğ´ĞµĞºÑÑ‹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº: {list(problematic_indices[:20])}{'...' if len(problematic_indices) > 20 else ''}")

# Ğ£Ğ”ĞĞ›Ğ¯Ğ•Ğœ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸
df_contract = df_contract.drop(index=problematic_indices)


# ĞœĞµĞ½ÑĞµĞ¼ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 
df_contract_retype = {
    'MonthlyCharges': 'float64',
    'TotalCharges': 'float64',
}
df_contract = df_contract.set_index('customerID')
df_contract = df_contract.astype(df_contract_retype)

df_contract['BeginDate'] = pd.to_datetime(df_contract['BeginDate'], errors='coerce')
df_contract['EndDate'] = pd.to_datetime(df_contract['EndDate'], errors='coerce')


# Ğ”Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ĞºÑ‚Ğ° 
end_date = pd.to_datetime('2020-02-01') # Ğ¿Ğ¾ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° 2020-02-01 Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ÑÑ Ğ´Ğ°Ñ‚Ğ° 
df_contract['duration_contract'] = (end_date - df_contract['BeginDate']).dt.days

# Ğ¢Ğ°Ñ€Ğ³ĞµÑ‚ 
df_contract['target'] = df_contract['EndDate'].notna().astype(int)

df_contract = df_contract.drop(['BeginDate', 'EndDate'], axis=1)


df_contract['target'].value_counts(normalize=True)


df_contract.head()


df_contract.info()


df_contract.describe().T


df_contract.pivot_table(index='Type', values='MonthlyCharges')


for col in df_contract.select_dtypes(include=['int', 'float']).columns:
    plot_histogram_with_boxplot(df_contract, col, 'target')

for col in df_contract.select_dtypes(include=['object', 'string']).columns:
    plot_pivot_pie_chart(df_contract, col)








df_personal.head()


df_personal.info()


preprocess(df_personal)


# ĞœĞµĞ½ÑĞµĞ¼ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¸Ğ½Ğ´ĞµĞºÑ 
df_personal = df_personal.set_index('customerID')
df_personal['SeniorCitizen'] = df_personal['SeniorCitizen'].astype('int32')


df_personal.head()


df_personal.info()


df_personal.describe().T


for col in df_personal.select_dtypes(include=['object', 'string', 'int']).columns:
    plot_pivot_pie_chart(df_personal, col)








df_internet.head()


df_internet.info()


preprocess(df_internet)


df_internet = df_internet.set_index('customerID')


df_internet.head()


df_internet.describe().T


for col in df_personal.select_dtypes(include=['object', 'string', 'int']).columns:
    plot_pivot_pie_chart(df_personal, col)








df_phone.head()


df_phone.info()





preprocess(df_phone)


df_phone = df_phone.rename(columns={'CustomerId': 'customerID'})
df_phone = df_phone.set_index('customerID')


df_phone.head()


plot_pivot_pie_chart(df_phone, 'MultipleLines')








df_contract_merged = df_contract.reset_index()

# ĞŸĞ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ
df_full = (df_contract_merged
           .merge(df_personal, on='customerID', how='left')
           .merge(df_internet, on='customerID', how='left')
           .merge(df_phone, on='customerID', how='left'))
df_full.head()


df_full.info()


preprocess(df_full)


# Ğ˜Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚-ÑƒÑĞ»ÑƒĞ³Ğ¸ (ĞºĞ»Ğ¸ĞµĞ½Ñ‚ Ğ½Ğµ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡Ğ¸Ğ» Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚)
internet_columns = [
    'InternetService', 'OnlineSecurity', 'OnlineBackup', 
    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies'
]
df_full[internet_columns] = df_full[internet_columns].fillna('No')

# Ğ¢ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸ (ĞºĞ»Ğ¸ĞµĞ½Ñ‚ Ğ½Ğµ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡Ğ¸Ğ» Ğ´Ğ¾Ğ¿. Ğ»Ğ¸Ğ½Ğ¸Ğ¸)
df_full['MultipleLines'] = df_full['MultipleLines'].fillna('No')

df_full.isna().sum()


# Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ ÑÑ‚Ğ¾Ğ»Ğ±ĞµÑ† customerID 
df_full = df_full.drop('customerID', axis=1)


df_full.duplicated().sum()


df_full = df_full.drop_duplicates()


df_full.profile_report(correlations=None, interactions=None).to_file('df_full_report.html')








# ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸:
phik_overview = df_full.phik_matrix(interval_cols=['MonthlyCharges', 'TotalCharges', 'duration_cont'])
phik_overview.round(2)

plot_correlation_matrix(phik_overview.values, 
                        x_labels=phik_overview.columns, 
                        y_labels=phik_overview.index, 
                        vmin=0, vmax=1, color_map="Blues", 
                        title=r"ĞšĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ $\phi_K$", 
                        fontsize_factor=0.8, 
                        figsize=(12, 10))
plt.tight_layout()
plt.show()





# ĞšĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸:
for i in df_full.columns:
    for j in df_full:
        if i == j:
            print(f'Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ° "{i}"')
                
            t0 = df_full.loc[df_full['target'] == 0, i] 
            t1 = df_full.loc[df_full['target'] == 1, i] 
                
            t0 = t0.rename('ĞšĞ»Ğ°ÑÑ 0') 
            t1 = t1.rename('ĞšĞ»Ğ°ÑÑ 1') 
                
            display(pd.DataFrame([t0.describe(), t1.describe()])) 
                
            t0.hist(legend=True, alpha=.8, figsize=(15, 10))
            t1.hist(legend=True, alpha=.8) 
            plt.title(f'Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ° "{i}"')
            plt.xlabel(f'Ğ—Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ° "{i}"')
            plt.ylabel('ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹')
            plt.show()











X = df_full.drop('target', axis=1)
y = df_full['target']

# 1. 75/25 â†’ train+valid / test
X_train, X_test, y_train, y_test = train_test_split(
    X, 
    y, 
    test_size=0.25, 
    random_state=RANDOM_STATE, 
    stratify=y, 
    shuffle=True
)

# âœ… ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ¬ĞĞ«Ğ• print
print(f'ĞĞ±Ñ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€:        {X.shape}')
print(f'Train:               ({X_train.shape[0]/X.shape[0]:.1%})')
print(f'Test:                ({X_test.shape[0]/X.shape[0]:.1%})')








ohe_columns = [
    'Type', 'PaperlessBilling', 'PaymentMethod', 
    'gender', 'Partner', 'Dependents', 'InternetService', 
    'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',
    'TechSupport', 'StreamingTV', 'StreamingMovies',
    'MultipleLines', 
]

num_columns = ['MonthlyCharges', 'TotalCharges', 'duration_contract', 'SeniorCitizen', ]


ohe_pipe = Pipeline(
    [
        ('simpleImputer_ohe', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),
        ('ohe', OneHotEncoder(handle_unknown='error', sparse_output=False, drop='first'))
    ]
)


data_preprocessor = ColumnTransformer(
    [('ohe', ohe_pipe, ohe_columns),
     ('num', MinMaxScaler(), num_columns)
    ], 
    remainder='passthrough'
)


pipe_final= Pipeline(
    [
        ('preprocessor', data_preprocessor),
        ('models', RandomForestClassifier(random_state=RANDOM_STATE))
    ]
)


param_grid = [
    # 1. RandomForest (Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹, ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹)
    {
        'models': [RandomForestClassifier(random_state=RANDOM_STATE)],
        'models__n_estimators': [200, 500, 1000],
        'models__max_depth': [10, 15, None],
        'models__min_samples_split': [2, 5, 10],
        'models__max_features': ['sqrt', 'log2'],
        'preprocessor__num': [StandardScaler(), MinMaxScaler(), 'passthrough']
    },

    # 2. CatBoost (Ğ»Ğ¸Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…!)
    {
        'models': [CatBoostClassifier(
            random_state=RANDOM_STATE, 
            verbose=0,  
            eval_metric='AUC'
        )],
        'models__iterations': [200, 500, 1000],
        'models__learning_rate': [0.01, 0.05, 0.1],
        'models__depth': [6, 8, 10],
        'models__l2_leaf_reg': [1, 3, 5],
        'preprocessor__num': [StandardScaler(), MinMaxScaler(), 'passthrough']
    },

    # 3. LogisticRegression (Ğ±Ğ°Ğ·Ğ»Ğ°Ğ¹Ğ½, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ)
    {
        'models': [LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)],
        'models__C': np.logspace(-3, 2, 6),
        'models__penalty': ['l1', 'l2'],
        'models__solver': ['liblinear', 'lbfgs'],
        'preprocessor__num': [StandardScaler(), MinMaxScaler(), 'passthrough']
    }
]



random = RandomizedSearchCV(
    pipe_final, 
    param_grid, 
    cv=10, 
    scoring='roc_auc', 
    random_state=RANDOM_STATE,
    n_jobs=-1
)


random.fit(X_train, y_train)


print('=== Ğ›Ğ£Ğ§Ğ¨ĞĞ¯ ĞœĞĞ”Ğ•Ğ›Ğ¬ ===')
print(f"Ğ¢Ğ¸Ğ¿ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {random.best_estimator_.named_steps['models'].__class__.__name__}")
print(f"ROC-AUC (CV): {random.best_score_:.3f}")
print("\nĞ›ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹:")
best_params = random.best_params_
for key, value in best_params.items():
    if 'models__' in key:
        param_name = key.replace('models__', '')
        print(f"  {param_name}: {value}")
    else:
        print(f"  {key}: {value}")








# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° MPS
device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
print(f"âœ… Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: {device}")


# Ğ ĞĞ—Ğ”Ğ•Ğ›Ğ•ĞĞ˜Ğ• Ğ½Ğ° train/val 
X_train_full_tr = data_preprocessor.fit_transform(X_train).astype(np.float32)
y_train_full = y_train.values

# 80% train, 20% val
X_train_tr, X_val_tr, y_train_tr, y_val_tr = train_test_split(
    X_train_full_tr, y_train_full, test_size=0.2, random_state=RANDOM_STATE, stratify=y_train_full
)

print(f"ğŸ“Š Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº (train/val):")
print(f"Train: {X_train_tr.shape} | Val: {X_val_tr.shape}")


# ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ 
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_tr)
X_val_scaled = scaler.transform(X_val_tr)

# PyTorch Datasets 
train_dataset = TensorDataset(torch.FloatTensor(X_train_scaled), torch.FloatTensor(y_train_tr))
val_dataset = TensorDataset(torch.FloatTensor(X_val_scaled), torch.FloatTensor(y_val_tr))

train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=256)


# ĞœĞ¾Ğ´ĞµĞ»ÑŒ, optimizer, Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ 
class ChurnNet(nn.Module):
    def __init__(self, input_size):
        super(ChurnNet, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_size, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(64, 32), nn.BatchNorm1d(32), nn.ReLU(), nn.Dropout(0.2),
            nn.Linear(32, 1), nn.Sigmoid()
        )
    def forward(self, x):
        return self.layers(x)

input_size = X_train_scaled.shape[1]
model = ChurnNet(input_size).to(device)
print(f"ğŸ“Š ĞœĞ¾Ğ´ĞµĞ»ÑŒ: {input_size} â†’ 128 â†’ 64 â†’ 32 â†’ 1")

criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)

def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    for X_batch, y_batch in loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        outputs = model(X_batch).squeeze()
        loss = criterion(outputs, y_batch)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)

def evaluate(model, loader, criterion, device):
    model.eval()
    total_loss = 0
    all_probas, all_labels = [], []
    with torch.no_grad():
        for X_batch, y_batch in loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch).squeeze()
            loss = criterion(outputs, y_batch)
            total_loss += loss.item()
            all_probas.extend(outputs.cpu().numpy())
            all_labels.extend(y_batch.cpu().numpy())
    auc = roc_auc_score(all_labels, all_probas)
    return total_loss / len(loader), auc


# 7. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
print("\nğŸš€ ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° MPS (train/val)...")
history = {'train_loss': [], 'val_loss': [], 'val_auc': []}

best_val_auc = 0
patience_counter = 0
MAX_EPOCHS = 200

for epoch in range(MAX_EPOCHS):
    # Train
    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
    
    # Validation 
    val_loss, val_auc = evaluate(model, val_loader, criterion, device)
    
    scheduler.step(val_loss)
    
    history['train_loss'].append(train_loss)
    history['val_loss'].append(val_loss)
    history['val_auc'].append(val_auc)
    
    # Early Stopping Ğ¿Ğ¾ val_auc
    if val_auc > best_val_auc:
        best_val_auc = val_auc
        patience_counter = 0
        torch.save(model.state_dict(), 'best_model_mps.pth')
    else:
        patience_counter += 1
    
    if (epoch + 1) % 10 == 0:
        print(f"Ğ­Ğ¿Ğ¾Ñ…Ğ° {epoch+1:2d}: Train={train_loss:.3f}, Val={val_loss:.3f}, "
              f"Val_AUC={val_auc:.3f}, Patience={patience_counter}")

    if patience_counter >= 30:
        print(f"ğŸ›‘ Early stopping Ğ½Ğ° ÑĞ¿Ğ¾Ñ…Ğµ {epoch+1}")
        break


# Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ¿Ğ¾ val
model.load_state_dict(torch.load('best_model_mps.pth', map_location=device))
final_val_loss, final_val_auc = evaluate(model, val_loader, criterion, device)

print(f"\nğŸ¯ Ğ›Ğ£Ğ§Ğ¨Ğ˜Ğ• Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ« (Ğ½Ğ° val):")
print(f"Val Loss:  {final_val_loss:.3f}")
print(f"Val AUC:   {final_val_auc:.3f}")





print(f"NN Model AUC:       {final_val_auc:.3f}")
print(f"RandomSearchCV AUC: {random.best_score_:.3f}")








y_test_pred = random.predict(X_test)
y_test_proba = random.predict_proba(X_test)[:, 1]  # Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»Ğ°ÑÑĞ° 1

print('ROC-AUC:   ', round(roc_auc_score(y_test, y_test_proba), 3))
print('F1-score:  ', round(f1_score(y_test, y_test_pred), 3))
print('Accuracy:  ', round(accuracy_score(y_test, y_test_pred), 3))
print('Precision: ', round(precision_score(y_test, y_test_pred), 3))
print('Recall:    ', round(recall_score(y_test, y_test_pred), 3))

# Ğ“Ğ ĞĞ¤Ğ˜ĞšĞ˜ ROC-AUC + Precision-Recall
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# 1. ROC-AUC ĞšĞ Ğ˜Ğ’ĞĞ¯
fpr, tpr, _ = roc_curve(y_test, y_test_proba)
roc_auc = roc_auc_score(y_test, y_test_proba)

RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name="ĞœĞ¾Ğ´ĞµĞ»ÑŒ").plot(ax=ax1)
ax1.plot([0, 1], [0, 1], 'r--', label='Ğ¡Ğ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€')
ax1.set_title(f'ROC-ĞšÑ€Ğ¸Ğ²Ğ°Ñ (AUC = {roc_auc:.3f})')
ax1.legend(loc='lower right')
ax1.grid(True, alpha=0.3)

# 2. Precision-Recall ĞšĞ Ğ˜Ğ’ĞĞ¯
precision, recall, _ = precision_recall_curve(y_test, y_test_proba)

PrecisionRecallDisplay(precision=precision, recall=recall).plot(ax=ax2)
ax2.set_title('Precision-Recall ĞšÑ€Ğ¸Ğ²Ğ°Ñ')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()





cm = confusion_matrix(y_test, y_test_pred)
fig, ax = plt.subplots(figsize=(8, 6))
ax.grid(False)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['ĞÑÑ‚Ğ°Ğ»ÑÑ', 'Ğ£ÑˆÑ‘Ğ»'])
disp.plot(ax=ax, cmap='Blues')
plt.title('ĞœĞ°Ñ‚Ñ€Ğ¸Ñ†Ğ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº')
plt.show()








# Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ
y_test_proba = random.predict_proba(X_test)[:, 1]
y_pred = (y_test_proba >= 0.1).astype(int)

print('F1-score:  ', round(f1_score(y_test, y_pred), 5))
print('Accuracy:  ', round(accuracy_score(y_test, y_pred), 5))
print('Precision: ', round(precision_score(y_test, y_pred), 5))
print('Recall:    ', round(recall_score(y_test, y_pred), 5))

cm = confusion_matrix(y_test, y_pred)
print(cm)
fig, ax = plt.subplots(figsize=(8, 6))
ax.grid(False)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['ĞÑÑ‚Ğ°Ğ»ÑÑ', 'Ğ£ÑˆÑ‘Ğ»'])
disp.plot(ax=ax, cmap='Blues')
plt.title('ĞœĞ°Ñ‚Ñ€Ğ¸Ñ†Ğ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº')
plt.show()





# Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ CatBoost
model = random.best_estimator_.steps[-1][1]
importance_values = model.get_feature_importance()

# ĞŸĞĞ›Ğ£Ğ§ĞĞ•Ğœ ĞĞĞ¡Ğ¢ĞĞ¯Ğ©Ğ˜Ğ• Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ñ‡ĞµĞ¹ Ğ¸Ğ· preprocessor
preprocessor = random.best_estimator_.named_steps['preprocessor']


try:
    feature_names = preprocessor.get_feature_names_out()
    print("get_feature_names_out() Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚!")
except:
    # Ğ¡Ñ‚Ğ°Ñ€Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ´Ğ»Ñ ColumnTransformer
    feature_names = []
    for name, transformer, cols in preprocessor.transformers_:
        if hasattr(transformer, 'get_feature_names_out'):
            feature_names.extend(transformer.get_feature_names_out(cols))
        else:
            # Ğ”Ğ»Ñ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² (StandardScaler, etc.)
            feature_names.extend(cols)
    print("âœ… Ğ ÑƒÑ‡Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğ¹")

print(f"Ğ¤Ğ¸Ñ‡ĞµĞ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {len(importance_values)}")
print(f"ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¾: {len(feature_names)}")

# âœ… 3. DataFrame Ñ ĞĞĞ¡Ğ¢ĞĞ¯Ğ©Ğ˜ĞœĞ˜ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸
feature_importance = pd.DataFrame({
    'feature': feature_names[:len(importance_values)],  # ĞĞ±Ñ€ĞµĞ·Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€
    'importance': importance_values
}).sort_values('importance', ascending=False)

print("\nğŸ” Ğ¢ĞĞŸ-10 Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²:")
print(feature_importance.head(10).round(3))


# Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº Ñ Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸
plt.figure(figsize=(12, 8))
sns.barplot(data=feature_importance.head(10), y='feature', x='importance')
plt.title('Ğ¢Ğ¾Ğ¿-10 Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² (CatBoost)')
plt.xlabel('Ğ’Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ')
plt.tight_layout()
plt.show()





# Ğ˜Ğ¡Ğ¡Ğ›Ğ•Ğ”ĞĞ’ĞĞĞ˜Ğ• Ğ¢ĞĞŸ-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ° 
top_feature_processed = feature_importance.iloc[0]['feature']
importance_top = feature_importance.iloc[0]['importance']

# Ğ˜Ğ—Ğ’Ğ›Ğ•ĞšĞĞ•Ğœ ĞĞ Ğ˜Ğ“Ğ˜ĞĞĞ›Ğ¬ĞĞĞ• Ğ˜ĞœĞ¯
if '__' in top_feature_processed:
    top_feature_orig = top_feature_processed.split('__')[1]
else:
    top_feature_orig = top_feature_processed.split('_')[0]

print(f"\nğŸ“Š Ğ˜Ğ¡Ğ¡Ğ›Ğ•Ğ”Ğ£Ğ•Ğœ Ğ¢ĞĞŸ-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğº:")
print(f"ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğµ: '{top_feature_processed}' (Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ: {importance_top:.3})")
print(f"ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ: '{top_feature_orig}'")

# ĞŸĞ ĞĞ’Ğ•Ğ Ğ¯Ğ•Ğœ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ² X_test
if top_feature_orig in X_test.columns:
    
    # DataFrame Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
    df_analysis = X_test[[top_feature_orig]].copy()
    df_analysis['churn'] = y_test.values
    df_analysis['churn_label'] = df_analysis['churn'].map({0: 'ĞÑÑ‚Ğ°Ğ»ÑÑ', 1: 'Ğ£ÑˆÑ‘Ğ»'})
    
    # 3 Ğ“Ğ ĞĞ¤Ğ˜ĞšĞ - 
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    # 1. BOXPLOT
    sns.boxplot(data=df_analysis, x='churn_label', y=top_feature_orig, ax=axes[0])
    axes[0].set_title(f'{top_feature_orig}: ĞÑÑ‚Ğ°Ğ»ÑÑ vs Ğ£ÑˆÑ‘Ğ»')
    
    # 2. Ğ“Ğ˜Ğ¡Ğ¢ĞĞ“Ğ ĞĞœĞœĞ« Ğ¿Ğ¾ ĞºĞ»Ğ°ÑÑĞ°Ğ¼
    for churn in [0, 1]:
        subset = df_analysis[df_analysis['churn'] == churn]
        axes[1].hist(subset[top_feature_orig], alpha=0.6, bins=30, 
                    label=f'Ğ£ÑˆÑ‘Ğ» (churn={churn})')
    axes[1].set_xlabel(top_feature_orig)
    axes[1].set_ylabel('Ğ§Ğ°ÑÑ‚Ğ¾Ñ‚Ğ°')
    axes[1].set_title(f'Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ {top_feature_orig}')
    axes[1].legend()
    
    # 3. Ğ”ĞĞ›Ğ¯ ĞĞ¢Ğ¢ĞĞšĞ Ğ¿Ğ¾ Ğ´ĞµÑ†Ğ¸Ğ»ÑĞ¼ 
    df_analysis['decile_num'] = pd.qcut(df_analysis[top_feature_orig], q=10, duplicates='drop', labels=False)
    churn_rate = df_analysis.groupby('decile_num')['churn'].mean().reset_index()
    
    axes[2].plot(churn_rate['decile_num'], churn_rate['churn'], marker='o', linewidth=3)
    axes[2].set_xlabel(f'{top_feature_orig} (Ğ´ĞµÑ†Ğ¸Ğ»ÑŒ 0-9)')
    axes[2].set_ylabel('Ğ”Ğ¾Ğ»Ñ Ğ¾Ñ‚Ñ‚Ğ¾ĞºĞ°')
    axes[2].set_title(f'Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ñ‚Ğ¾ĞºĞ° Ğ¾Ñ‚ {top_feature_orig}')
    axes[2].grid(True, alpha=0.3)
    
    # ĞŸĞ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğ½Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞµ - Ğ§Ğ˜Ğ¡Ğ›Ğ 
    for i, row in churn_rate.iterrows():
        axes[2].annotate(f'{row["churn"]:.1%}', 
                        (row['decile_num'], row['churn']), 
                        xytext=(0, 5), textcoords='offset points', ha='center')
    
    plt.tight_layout()
    plt.show()
    
else:
    print(f"âŒ '{top_feature_orig}' ĞĞ• ĞĞĞ™Ğ”Ğ•Ğ")
    print("ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğº...")
    numeric_features = X_test.select_dtypes(include=[np.number]).columns
    if len(numeric_features) > 0:
        top_feature_orig = numeric_features[0]
        print(f"ğŸ”„ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼: '{top_feature_orig}'")
        # ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ´ Ğ²Ñ‹ÑˆĞµ...







