














import pandas as pd
from ydata_profiling import ProfileReport
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sqlalchemy import create_engine, text
import os

from sklearn.model_selection import (
    train_test_split, GridSearchCV, RandomizedSearchCV, 
    cross_val_score, cross_validate
)
from sklearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import (
    StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder
)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from catboost import CatBoostClassifier

from sklearn.metrics import (
    roc_auc_score, f1_score, accuracy_score, precision_score, recall_score,
    confusion_matrix, ConfusionMatrixDisplay, classification_report,
    roc_curve, precision_recall_curve, RocCurveDisplay, PrecisionRecallDisplay
)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

import phik
from phik import phik_matrix
from phik.report import plot_correlation_matrix

from scipy.stats import chi2_contingency
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

print("‚úÖ –í—Å–µ –∏–º–ø–æ—Ä—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã")


path_to_db = 'ds-plus-final.db'
engine = create_engine(f'sqlite:///{path_to_db}', echo=False)

RANDOM_STATE = 20226 








# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ç–∞–±–ª–∏—Ü –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
def check_tables():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ç–∞–±–ª–∏—Ü –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
    try:
        # –ó–∞–ø—Ä–æ—Å –∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π —Å—Ö–µ–º–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ —Ç–∞–±–ª–∏—Ü
        query = """
        SELECT name, type 
        FROM sqlite_master 
        WHERE type IN ('table', 'view')
        ORDER BY type, name;
        """
        
        with engine.connect() as conn:
            result = conn.execute(text(query))
            tables = [row[0] for row in result]
            
        print("–¢–∞–±–ª–∏—Ü—ã –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö:")
        for table in tables:
            print(f"- {table}")
            
        return tables
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ç–∞–±–ª–∏—Ü: {e}")
        return []

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞–±–ª–∏—Ü—ã –ø–µ—Ä–µ–¥ —Ä–∞–±–æ—Ç–æ–π —Å –Ω–∏–º–∏
available_tables = check_tables()

print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(available_tables)} —Ç–∞–±–ª–∏—Ü(—ã)")





tables = ['contract', 'personal', 'internet', 'phone']
dataframes = {}

for table in tables:
    query = f'''
    SELECT *
    FROM {table}
    '''
    df = pd.read_sql_query(query, con=engine) # , index_col='customerID'
    dataframes[f'df_{table}'] = df  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å





df_contract = dataframes['df_contract']
df_personal = dataframes['df_personal']
df_internet = dataframes['df_internet']
df_phone = dataframes['df_phone']











# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∞–Ω–Ω—ã—Ö
def preprocess(df):
    print('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤:')
    print(df.isna().sum())
    print('\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —è–≤–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: ', df.duplicated().sum())
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ø–∏–∏ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö 
    df_copy = df.copy()

    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–µ—è–≤–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    for col in df_copy.select_dtypes(include=['object', 'category']).columns:
        df_copy[col] = df_copy[col].str.lower().str.replace(' ', '', regex=False)

    print(f'–ö–æ–ª–∏—á–µ—Å—Ç–æ–≤ –Ω–µ—è–≤–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {df_copy.duplicated().sum()}\n')

    # –£–¥–∞–ª—è–µ–º –∫–æ–ø–∏—é –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    del df_copy

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –∫–æ—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    for col in df.select_dtypes(include=['object', 'category']).columns:
        print(f'–°—Ç–æ–ª–±–µ—Ü: {col} \n–∑–Ω–∞—á–µ–Ω–∏—è —Å—Ç–æ–ª–±—Ü–∞: {df[col].unique()}\n')
    

def plot_histogram_with_boxplot(df, col, target=None):
    """
    –°—Ç—Ä–æ–∏—Ç –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É –∏ boxplot –¥–ª—è —á–∏—Å–ª–æ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞.
    
    Parameters:
    df (pandas.DataFrame): –î–∞—Ç–∞—Å–µ—Ç.
    col (str): –ù–∞–∑–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞.
    target (str, optional): –ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –¥–ª—è —Ä–∞—Å–∫—Ä–∞—Å–∫–∏.
    """
    sns.set()
    fig, axes = plt.subplots(1, 2, figsize=(16, 4))
    
    # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞
    axes[0].set_title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ {col}', fontsize=16)
    axes[0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', fontsize=14)
    sns.histplot(df, bins=20, kde=True, ax=axes[0], hue=target, x=col)
    
    # Boxplot
    axes[1].set_title(f'–Ø—â–∏–∫ —Å —É—Å–∞–º–∏ –¥–ª—è {col}', fontsize=16)
    sns.boxplot(data=df, ax=axes[1], y=col)
    axes[1].set_ylabel(col, fontsize=14)
    
    plt.show()


def plot_pivot_pie_chart(df, col):
    """
    –°—Ç—Ä–æ–∏—Ç –∫—Ä—É–≥–æ–≤—É—é –¥–∏–∞–≥—Ä–∞–º–º—É —á–∞—Å—Ç–æ—Ç—ã –∑–Ω–∞—á–µ–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–∞.
    
    Parameters:
    df (pandas.DataFrame): –î–∞—Ç–∞—Å–µ—Ç.
    col (str): –ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞.
    """
    value_counts = df[col].value_counts()
    plt.figure(figsize=(8, 8))
    plt.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%', startangle=140)
    plt.title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ {col}', fontsize=16)
    plt.show()








df_contract.head()


df_contract.info() 



preprocess(df_contract)


# –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–±–ª–µ–º–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –≤ TotalCharges
problematic_mask = ~pd.to_numeric(df_contract['TotalCharges'], errors='coerce').notna()
problematic_indices = df_contract[problematic_mask].index

print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(problematic_indices)} —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–±–ª–µ–º–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏:")
print(f"–ò–Ω–¥–µ–∫—Å—ã –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {list(problematic_indices[:20])}{'...' if len(problematic_indices) > 20 else ''}")

display(df_contract.loc[problematic_indices])





df_contract['TotalCharges'].loc[problematic_indices] = df_contract['MonthlyCharges'].loc[problematic_indices]
display(df_contract.loc[problematic_indices])








# –ú–µ–Ω—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö 
df_contract_retype = {
    'MonthlyCharges': 'float64',
    'TotalCharges': 'float64',
}
df_contract = df_contract.set_index('customerID')
df_contract = df_contract.astype(df_contract_retype)

df_contract['BeginDate'] = pd.to_datetime(df_contract['BeginDate'], errors='coerce')
df_contract['EndDate'] = pd.to_datetime(df_contract['EndDate'], errors='coerce')


# –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ 
snapshot_date = pd.to_datetime('2020-02-01')

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º EndDate: –¥–∞—Ç—ã ‚Üí datetime, 'No' ‚Üí NaT
df_contract['EndDate'] = pd.to_datetime(
    df_contract['EndDate'].replace('No', pd.NA)
)

# –í—ã–±–∏—Ä–∞–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫—É—é –∫–æ–Ω–µ—á–Ω—É—é –¥–∞—Ç—É:
# –µ—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç —É—à—ë–ª ‚Üí EndDate, –µ—Å–ª–∏ –Ω–µ—Ç ‚Üí snapshot_date
effective_end = df_contract['EndDate'].fillna(snapshot_date)

df_contract['duration_contract'] = (effective_end - df_contract['BeginDate']).dt.days

# –¢–∞—Ä–≥–µ—Ç 
df_contract['target'] = df_contract['EndDate'].notna().astype(int)

df_contract = df_contract.drop(['BeginDate', 'EndDate'], axis=1)


df_contract.head()








df_contract['target'].value_counts(normalize=True)





df_contract.head()


df_contract.info()


df_contract.describe().T


df_contract.pivot_table(index='Type', values='MonthlyCharges')


for col in df_contract.select_dtypes(include=['int', 'float']).columns:
    plot_histogram_with_boxplot(df_contract, col, 'target')

for col in df_contract.select_dtypes(include=['object', 'string']).columns:
    plot_pivot_pie_chart(df_contract, col)








df_personal.head()


df_personal.info()


preprocess(df_personal)


# –ú–µ–Ω—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å 
df_personal = df_personal.set_index('customerID')
df_personal['SeniorCitizen'] = df_personal['SeniorCitizen'].astype('int32')


df_personal.head()


df_personal.info()


df_personal.describe().T


for col in df_personal.select_dtypes(include=['object', 'string', 'int']).columns:
    plot_pivot_pie_chart(df_personal, col)








df_internet.head()


df_internet.info()


preprocess(df_internet)


df_internet = df_internet.set_index('customerID')


df_internet.head()


df_internet.describe().T


for col in df_personal.select_dtypes(include=['object', 'string', 'int']).columns:
    plot_pivot_pie_chart(df_personal, col)








df_phone.head()


df_phone.info()





preprocess(df_phone)


df_phone = df_phone.rename(columns={'CustomerId': 'customerID'})
df_phone = df_phone.set_index('customerID')


df_phone.head()


plot_pivot_pie_chart(df_phone, 'MultipleLines')








df_contract_merged = df_contract.reset_index()

# –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
df_full = (df_contract_merged
           .merge(df_personal, on='customerID', how='left')
           .merge(df_internet, on='customerID', how='left')
           .merge(df_phone, on='customerID', how='left'))
df_full.head()





df_full.info()


preprocess(df_full)


# –ò–Ω—Ç–µ—Ä–Ω–µ—Ç-—É—Å–ª—É–≥–∏ (–∫–ª–∏–µ–Ω—Ç –Ω–µ –ø–æ–¥–∫–ª—é—á–∏–ª –∏–Ω—Ç–µ—Ä–Ω–µ—Ç)
internet_columns = [
    'InternetService', 'OnlineSecurity', 'OnlineBackup', 
    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies'
]
df_full[internet_columns] = df_full[internet_columns].fillna('No')

# –¢–µ–ª–µ—Ñ–æ–Ω–Ω—ã–µ –ª–∏–Ω–∏–∏ (–∫–ª–∏–µ–Ω—Ç –Ω–µ –ø–æ–¥–∫–ª—é—á–∏–ª –¥–æ–ø. –ª–∏–Ω–∏–∏)
df_full['MultipleLines'] = df_full['MultipleLines'].fillna('no_phone')

df_full.isna().sum()








# –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±–µ—Ü customerID 
df_full = df_full.drop('customerID', axis=1)


df_full.duplicated().sum()


df_full = df_full.drop_duplicates()


df_full.profile_report(correlations=None, interactions=None).to_file('df_full_report.html')








# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞—Ç—Ä–∏—Ü—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏:
def phik_matrix(df):

    phik_overview = df.phik_matrix(interval_cols=['MonthlyCharges', 'TotalCharges', 'duration_cont'])
    phik_overview.round(2)
    
    plot_correlation_matrix(phik_overview.values, 
                            x_labels=phik_overview.columns, 
                            y_labels=phik_overview.index, 
                            vmin=0, vmax=1, color_map="Blues", 
                            title=r"–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è $\phi_K$", 
                            fontsize_factor=0.8, 
                            figsize=(12, 10))
    plt.tight_layout()
    plt.show()
    return phik_overview

phik_values = phik_matrix(df_full)











# –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏:
for i in df_full.columns:
    for j in df_full:
        if i == j:
            print(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–∞ "{i}"')
                
            t0 = df_full.loc[df_full['target'] == 0, i] 
            t1 = df_full.loc[df_full['target'] == 1, i] 
                
            t0 = t0.rename('–ö–ª–∞—Å—Å 0') 
            t1 = t1.rename('–ö–ª–∞—Å—Å 1') 
                
            display(pd.DataFrame([t0.describe(), t1.describe()])) 
                
            t0.hist(legend=True, alpha=.8, figsize=(15, 10))
            t1.hist(legend=True, alpha=.8) 
            plt.title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–∞ "{i}"')
            plt.xlabel(f'–ó–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–∞ "{i}"')
            plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞—á–µ–Ω–∏–π')
            plt.show()











X = df_full.drop('target', axis=1)
y = df_full['target']

# 1. 75/25 ‚Üí train+valid / test
X_train, X_test, y_train, y_test = train_test_split(
    X, 
    y, 
    test_size=0.25, 
    random_state=RANDOM_STATE, 
    stratify=y, 
    shuffle=True
)

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–´–ï print
print(f'–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä:        {X.shape}')
print(f'Train:               ({X_train.shape[0]/X.shape[0]:.1%})')
print(f'Test:                ({X_test.shape[0]/X.shape[0]:.1%})')





preprocess(X)





ohe_columns = [
    'Type', 'PaperlessBilling', 'PaymentMethod', 
    'gender', 'Partner', 'Dependents',
    'InternetService','OnlineBackup', 'OnlineSecurity',
    'DeviceProtection', 
    'TechSupport', 'StreamingTV', 'StreamingMovies',
    'MultipleLines'
]

num_columns = ['MonthlyCharges', 'TotalCharges', 'duration_contract', 'SeniorCitizen']


ohe_pipe = Pipeline(
    [
        ('simpleImputer_ohe', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),
        ('ohe', OneHotEncoder(handle_unknown='error', sparse_output=False, drop='first'))
    ]
)


data_preprocessor = ColumnTransformer(
    [('ohe', ohe_pipe, ohe_columns),
     ('num', MinMaxScaler(), num_columns)
    ], 
    remainder='passthrough'
)


pipe_final= Pipeline(
    [
        ('preprocessor', data_preprocessor),
        ('models', RandomForestClassifier(random_state=RANDOM_STATE))
    ]
)


param_grid = [
    # 1. RandomForest (–±—ã—Å—Ç—Ä—ã–π, —Å—Ç–∞–±–∏–ª—å–Ω—ã–π)
    {
        'models': [RandomForestClassifier(random_state=RANDOM_STATE)],
        'models__n_estimators': [200, 500, 1000],
        'models__max_depth': [10, 15, None],
        'models__min_samples_split': [2, 5, 10],
        'models__max_features': ['sqrt', 'log2'],
        'preprocessor__num': [StandardScaler(), MinMaxScaler(), 'passthrough']
    },

    # 2. CatBoost (–ª–∏–¥–µ—Ä –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!)
    {
        'models': [CatBoostClassifier(
            random_state=RANDOM_STATE, 
            verbose=0,  
            eval_metric='AUC'
        )],
        'models__iterations': [200, 500, 1000],
        'models__learning_rate': [0.01, 0.05, 0.1],
        'models__depth': [6, 8, 10],
        'models__l2_leaf_reg': [1, 3, 5],
        'preprocessor__num': [StandardScaler(), MinMaxScaler(), 'passthrough']
    },

    # 3. LogisticRegression (–±–∞–∑–ª–∞–π–Ω, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å)
    {
        'models': [LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)],
        'models__C': np.logspace(-3, 2, 6),
        'models__penalty': ['l1', 'l2'],
        'models__solver': ['liblinear', 'lbfgs'],
        'preprocessor__num': [StandardScaler(), MinMaxScaler(), 'passthrough']
    }
]









random = RandomizedSearchCV(
    pipe_final, 
    param_grid, 
    cv=10, 
    scoring='roc_auc', 
    random_state=RANDOM_STATE,
    n_jobs=-1
)


random.fit(X_train, y_train)


print('=== –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨ ===')
print(f"–¢–∏–ø –º–æ–¥–µ–ª–∏: {random.best_estimator_.named_steps['models'].__class__.__name__}")
print(f"ROC-AUC (CV): {random.best_score_:.3f}")
print("\n–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
best_params = random.best_params_
for key, value in best_params.items():
    if 'models__' in key:
        param_name = key.replace('models__', '')
        print(f"  {param_name}: {value}")
    else:
        print(f"  {key}: {value}")








# –ü—Ä–æ–≤–µ—Ä–∫–∞ MPS
device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")


# –†–ê–ó–î–ï–õ–ï–ù–ò–ï –Ω–∞ train/val 
X_train_full_tr = data_preprocessor.fit_transform(X_train).astype(np.float32)
y_train_full = y_train.values

# 80% train, 20% val
X_train_tr, X_val_tr, y_train_tr, y_val_tr = train_test_split(
    X_train_full_tr, y_train_full, test_size=0.2, random_state=RANDOM_STATE, stratify=y_train_full
)

print(f"üìä –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–æ–∫ (train/val):")
print(f"Train: {X_train_tr.shape} | Val: {X_val_tr.shape}")


# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è 
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_tr)
X_val_scaled = scaler.transform(X_val_tr)

# PyTorch Datasets 
train_dataset = TensorDataset(torch.FloatTensor(X_train_scaled), torch.FloatTensor(y_train_tr))
val_dataset = TensorDataset(torch.FloatTensor(X_val_scaled), torch.FloatTensor(y_val_tr))

train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=256)


# –ú–æ–¥–µ–ª—å, optimizer, —Ñ—É–Ω–∫—Ü–∏–∏ 
class ChurnNet(nn.Module):
    def __init__(self, input_size):
        super(ChurnNet, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_size, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(64, 32), nn.BatchNorm1d(32), nn.ReLU(), nn.Dropout(0.2),
            nn.Linear(32, 1), nn.Sigmoid()
        )
    def forward(self, x):
        return self.layers(x)

input_size = X_train_scaled.shape[1]
model = ChurnNet(input_size).to(device)
print(f"üìä –ú–æ–¥–µ–ª—å: {input_size} ‚Üí 128 ‚Üí 64 ‚Üí 32 ‚Üí 1")

criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)

def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    for X_batch, y_batch in loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        outputs = model(X_batch).squeeze()
        loss = criterion(outputs, y_batch)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)

def evaluate(model, loader, criterion, device):
    model.eval()
    total_loss = 0
    all_probas, all_labels = [], []
    with torch.no_grad():
        for X_batch, y_batch in loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch).squeeze()
            loss = criterion(outputs, y_batch)
            total_loss += loss.item()
            all_probas.extend(outputs.cpu().numpy())
            all_labels.extend(y_batch.cpu().numpy())
    auc = roc_auc_score(all_labels, all_probas)
    return total_loss / len(loader), auc


# 7. –û–±—É—á–µ–Ω–∏–µ
print("\nüöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ MPS (train/val)...")
history = {'train_loss': [], 'val_loss': [], 'val_auc': []}

best_val_auc = 0
patience_counter = 0
MAX_EPOCHS = 200

for epoch in range(MAX_EPOCHS):
    # Train
    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
    
    # Validation 
    val_loss, val_auc = evaluate(model, val_loader, criterion, device)
    
    scheduler.step(val_loss)
    
    history['train_loss'].append(train_loss)
    history['val_loss'].append(val_loss)
    history['val_auc'].append(val_auc)
    
    # Early Stopping –ø–æ val_auc
    if val_auc > best_val_auc:
        best_val_auc = val_auc
        patience_counter = 0
        torch.save(model.state_dict(), 'best_model_mps.pth')
    else:
        patience_counter += 1
    
    if (epoch + 1) % 10 == 0:
        print(f"–≠–ø–æ—Ö–∞ {epoch+1:2d}: Train={train_loss:.3f}, Val={val_loss:.3f}, "
              f"Val_AUC={val_auc:.3f}, Patience={patience_counter}")

    if patience_counter >= 30:
        print(f"üõë Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
        break


# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¢–û–õ–¨–ö–û –ø–æ val
model.load_state_dict(torch.load('best_model_mps.pth', map_location=device))
final_val_loss, final_val_auc = evaluate(model, val_loader, criterion, device)

print(f"\nüéØ –õ–£–ß–®–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ (–Ω–∞ val):")
print(f"Val Loss:  {final_val_loss:.3f}")
print(f"Val AUC:   {final_val_auc:.3f}")





print(f"NN Model AUC:       {final_val_auc:.3f}")
print(f"RandomSearchCV AUC: {random.best_score_:.3f}")








y_test_pred = random.predict(X_test)
y_test_proba = random.predict_proba(X_test)[:, 1]  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∞ 1

print('ROC-AUC:   ', round(roc_auc_score(y_test, y_test_proba), 3))
print('F1-score:  ', round(f1_score(y_test, y_test_pred), 3))
print('Accuracy:  ', round(accuracy_score(y_test, y_test_pred), 3))
print('Precision: ', round(precision_score(y_test, y_test_pred), 3))
print('Recall:    ', round(recall_score(y_test, y_test_pred), 3))

# –ì–†–ê–§–ò–ö–ò ROC-AUC + Precision-Recall
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# 1. ROC-AUC –ö–†–ò–í–ê–Ø
fpr, tpr, _ = roc_curve(y_test, y_test_proba)
roc_auc = roc_auc_score(y_test, y_test_proba)

RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name="–ú–æ–¥–µ–ª—å").plot(ax=ax1)
ax1.plot([0, 1], [0, 1], 'r--', label='–°–ª—É—á–∞–π–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä')
ax1.set_title(f'ROC-–ö—Ä–∏–≤–∞—è (AUC = {roc_auc:.3f})')
ax1.legend(loc='lower right')
ax1.grid(True, alpha=0.3)

# 2. Precision-Recall –ö–†–ò–í–ê–Ø
precision, recall, _ = precision_recall_curve(y_test, y_test_proba)

PrecisionRecallDisplay(precision=precision, recall=recall).plot(ax=ax2)
ax2.set_title('Precision-Recall –ö—Ä–∏–≤–∞—è')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()





cm = confusion_matrix(y_test, y_test_pred)
fig, ax = plt.subplots(figsize=(8, 6))
ax.grid(False)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['–û—Å—Ç–∞–ª—Å—è', '–£—à—ë–ª'])
disp.plot(ax=ax, cmap='Blues')
plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')
plt.show()








# –ë–∞–∑–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_test_proba = random.predict_proba(X_test)[:, 1]
y_pred = (y_test_proba >= 0.1).astype(int)

print('ROC-AUC:   ', round(roc_auc_score(y_test, y_test_proba), 3))
print('F1-score:  ', round(f1_score(y_test, y_pred), 5))
print('Accuracy:  ', round(accuracy_score(y_test, y_pred), 5))
print('Precision: ', round(precision_score(y_test, y_pred), 5))
print('Recall:    ', round(recall_score(y_test, y_pred), 5))

cm = confusion_matrix(y_test, y_pred)
print(cm)
fig, ax = plt.subplots(figsize=(8, 6))
ax.grid(False)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['–û—Å—Ç–∞–ª—Å—è', '–£—à—ë–ª'])
disp.plot(ax=ax, cmap='Blues')
plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')
plt.show()





# –ò–∑–≤–ª–µ–∫–∞–µ–º CatBoost
model = random.best_estimator_.steps[-1][1]
importance_values = model.get_feature_importance()

# –ü–û–õ–£–ß–ê–ï–ú –ù–ê–°–¢–û–Ø–©–ò–ï –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∏—á–µ–π –∏–∑ preprocessor
preprocessor = random.best_estimator_.named_steps['preprocessor']


try:
    feature_names = preprocessor.get_feature_names_out()
    print("get_feature_names_out() —Ä–∞–±–æ—Ç–∞–µ—Ç!")
except:
    # –°—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–± –¥–ª—è ColumnTransformer
    feature_names = []
    for name, transformer, cols in preprocessor.transformers_:
        if hasattr(transformer, 'get_feature_names_out'):
            feature_names.extend(transformer.get_feature_names_out(cols))
        else:
            # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (StandardScaler, etc.)
            feature_names.extend(cols)
    print("‚úÖ –†—É—á–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π")

print(f"–§–∏—á–µ–π –≤ –º–æ–¥–µ–ª–∏: {len(importance_values)}")
print(f"–ù–∞–∑–≤–∞–Ω–∏–π –ø–æ–ª—É—á–µ–Ω–æ: {len(feature_names)}")

# ‚úÖ 3. DataFrame —Å –ù–ê–°–¢–û–Ø–©–ò–ú–ò –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
feature_importance = pd.DataFrame({
    'feature': feature_names[:len(importance_values)],  # –û–±—Ä–µ–∑–∞–µ–º –ø–æ–¥ —Ä–∞–∑–º–µ—Ä
    'importance': importance_values
}).sort_values('importance', ascending=False)

print("\nüîù –¢–û–ü-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
print(feature_importance.head(10).round(3))


# –ì—Ä–∞—Ñ–∏–∫ —Å –Ω–∞—Å—Ç–æ—è—â–∏–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
plt.figure(figsize=(12, 8))
sns.barplot(data=feature_importance.head(10), y='feature', x='importance')
plt.title('–¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (CatBoost)')
plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')
plt.tight_layout()
plt.show()





# –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï –¢–û–ü-–ø—Ä–∏–∑–Ω–∞–∫–∞ 
top_feature_processed = feature_importance.iloc[0]['feature']
importance_top = feature_importance.iloc[0]['importance']

# –ò–ó–í–õ–ï–ö–ê–ï–ú –û–†–ò–ì–ò–ù–ê–õ–¨–ù–û–ï –ò–ú–Ø
if '__' in top_feature_processed:
    top_feature_orig = top_feature_processed.split('__')[1]
else:
    top_feature_orig = top_feature_processed.split('_')[0]

print(f"\nüìä –ò–°–°–õ–ï–î–£–ï–ú –¢–û–ü-–ø—Ä–∏–∑–Ω–∞–∫:")
print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ: '{top_feature_processed}' (–≤–∞–∂–Ω–æ—Å—Ç—å: {importance_top:.3})")
print(f"–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ: '{top_feature_orig}'")

# –ü–†–û–í–ï–†–Ø–ï–ú –Ω–∞–ª–∏—á–∏–µ –≤ X_test
if top_feature_orig in X_test.columns:
    
    # DataFrame –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    df_analysis = X_test[[top_feature_orig]].copy()
    df_analysis['churn'] = y_test.values
    df_analysis['churn_label'] = df_analysis['churn'].map({0: '–û—Å—Ç–∞–ª—Å—è', 1: '–£—à—ë–ª'})
    
    # 3 –ì–†–ê–§–ò–ö–ê - 
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    # 1. BOXPLOT
    sns.boxplot(data=df_analysis, x='churn_label', y=top_feature_orig, ax=axes[0])
    axes[0].set_title(f'{top_feature_orig}: –û—Å—Ç–∞–ª—Å—è vs –£—à—ë–ª')
    
    # 2. –ì–ò–°–¢–û–ì–†–ê–ú–ú–´ –ø–æ –∫–ª–∞—Å—Å–∞–º
    for churn in [0, 1]:
        subset = df_analysis[df_analysis['churn'] == churn]
        axes[1].hist(subset[top_feature_orig], alpha=0.6, bins=30, 
                    label=f'–£—à—ë–ª (churn={churn})')
    axes[1].set_xlabel(top_feature_orig)
    axes[1].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    axes[1].set_title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {top_feature_orig}')
    axes[1].legend()
    
    # 3. –î–û–õ–Ø –û–¢–¢–û–ö–ê –ø–æ –¥–µ—Ü–∏–ª—è–º 
    df_analysis['decile_num'] = pd.qcut(df_analysis[top_feature_orig], q=10, duplicates='drop', labels=False)
    churn_rate = df_analysis.groupby('decile_num')['churn'].mean().reset_index()
    
    axes[2].plot(churn_rate['decile_num'], churn_rate['churn'], marker='o', linewidth=3)
    axes[2].set_xlabel(f'{top_feature_orig} (–¥–µ—Ü–∏–ª—å 0-9)')
    axes[2].set_ylabel('–î–æ–ª—è –æ—Ç—Ç–æ–∫–∞')
    axes[2].set_title(f'–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç—Ç–æ–∫–∞ –æ—Ç {top_feature_orig}')
    axes[2].grid(True, alpha=0.3)
    
    # –ü–æ–¥–ø–∏—Å–∏ –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ - –ß–ò–°–õ–ê 
    for i, row in churn_rate.iterrows():
        axes[2].annotate(f'{row["churn"]:.1%}', 
                        (row['decile_num'], row['churn']), 
                        xytext=(0, 5), textcoords='offset points', ha='center')
    
    plt.tight_layout()
    plt.show()
    
else:
    print(f"‚ùå '{top_feature_orig}' –ù–ï –ù–ê–ô–î–ï–ù")
    print("–ü—Ä–æ–±—É–µ–º –ø–µ—Ä–≤—ã–π —á–∏—Å–ª–æ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫...")
    numeric_features = X_test.select_dtypes(include=[np.number]).columns
    if len(numeric_features) > 0:
        top_feature_orig = numeric_features[0]
        print(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º: '{top_feature_orig}'")
        # –ü–æ–≤—Ç–æ—Ä–∏—Ç—å –∫–æ–¥ –≤—ã—à–µ...
















